\section{数学的知識}

\subsection{表記}

本書では基本的に実数しか扱いません．
そのため断りのない限り，すべて実数が使われることが前提になります．
表記としては，スカラーを$a \in \mathbb{R}$，$N$次元のベクトルを${\bf a} \in \mathbb{R}^{N}$，$N \times M$の行列を$A \in \mathbb{R}^{N \times M}$と表記します．





\subsection{ヤコビアン}

本書では主に{\bf 最適化}（Optimization）を利用していきます．
最適化とは，ある変数${\bf x} \in \mathbb{R}^{N}$に従う関数$f({\bf x})$（最適化に使われる関数はおおよそ{\bf コスト関数}（Cost Function）と呼ばれます）が定義されたときに，$f$の値を最小化（もしくは最大化）させること，またそれに対応する変数${\bf x}$を求めることになります．
最適化を行う方法には様々な方法がありますが，基本となることは，関数の勾配を求めることになります．
%
\begin{align}
  \frac{ \partial f({\bf x}) }{ \partial {\bf x} } =
  \left( \begin{matrix}
    \frac{ \partial f({\bf x}) }{ \partial x_{1} } &
    \cdots                                         &
    \frac{ \partial f({\bf x}) }{ \partial x_{N} }
  \end{matrix} \right)
  \label{eq:scholar_function_jacobian}
\end{align}
%
ベクトル関数${\bf f}({\bf x}) = \left( f_{1}({\bf x}) \cdots f_{M}({\bf x}) \right)^{\top} \in \mathbb{R}^{M}$に関してもヤコビアンを定めることができます．
%
\begin{align}
  \frac{ \partial {\bf f}({\bf x}) }{ \partial {\bf x} } =
  \left( \begin{matrix}
    \frac{ \partial f_{1}({\bf x}) }{ \partial x_{1} } &
    \cdots                                             &
    \frac{ \partial f_{1}({\bf x}) }{ \partial x_{N} } \\
    %
    \vdots                                             &
    \ddots                                             &
    \vdots                                             \\
    %
    \frac{ \partial f_{M}({\bf x}) }{ \partial x_{1} } &
    \cdots                                             &
    \frac{ \partial f_{M}({\bf x}) }{ \partial x_{N} } \\
  \end{matrix} \right)
  \label{eq:vector_function_jacobian}
\end{align}
%
なお本書では，基本的にヤコビアンを$J$と表記します．

ヤコビアンの計算は基本的に式(\ref{eq:scholar_function_jacobian})，(\ref{eq:vector_function_jacobian})に示す定義に従って行いますが，少し異なった方法でも導出することが可能です．
まず，関数$f({\bf x})$を$\delta {\bf x}$だけ変化させた結果をテイラー展開を用いて近似します．
%
\begin{align}
  f({\bf x} + \delta {\bf x}) \simeq
  f({\bf x}) + 
  J \delta {\bf x} + 
  \frac{1}{2} \delta {\bf x}^{\top} H \delta {\bf x}
  \label{eq:taylor_expansion_approximation_2nd_order}
\end{align}
%
なお$H = \frac{ \partial^{2} f({\bf x}) }{ \partial {\bf x}^{2} }$であり，これは{\bf ヘッセ行列}（Hessian）と呼ばれます．
ここで，2次の微小量を無視し，式(\ref{eq:taylor_expansion_approximation_2nd_order})の両辺が等しいと仮定すると，次式が得られます．
%
\begin{align}
  f({\bf x} + \delta {\bf x}) - f({\bf x}) = J \delta {\bf x}
  \label{eq:jacobian_difference}
\end{align}
%
すなわち，$f({\bf x} + \delta {\bf x})$と$f({\bf x})$の差分を$J \delta {\bf x}$という形で記述できるとヤコビアンを求めることができます．

例えば，$f(x) = x^{2}$という簡単な例を考えてみます．
この関数のヤコビアンは$\frac{ \partial x^{2} }{ \partial x } = 2x$となりますが，式(\ref{eq:jacobian_difference})に示す方法でヤコビアンを求めてみます．
%
\begin{align}
  \begin{split}
    & (x + \delta x)^{2} - x^{2} \\
    = & x^{2} + 2 x \delta x + \delta x^{2} - x^{2} \\
    = & 2 x \delta x
  \end{split}
  \label{eq:jacobian_difference_example}
\end{align}
%
ただし$\delta x^{2} \simeq 0$として2次の微小量を無視しました．
式(\ref{eq:jacobian_difference_example})に示すように，$f(x) = x^{2}$のヤコビアンを正しく求めることができました．
この方法を用いると，関数を直接微分しなくてもヤコビアンを求めることができます．

\subsection{ガウス・ニュートン法}

本書で扱う問題は，度々最適化問題に帰着されます．
最適化問題を解く際に用いられる方法は様々ありますが，本書では{\bf ガウス・ニュートン法}（Gauss-Newton Method）を用います．

ガウス・ニュートン法を考えるにあたり，まず状態変数${\bf x} \in \mathbb{R}^{N}$，およびこの状態に依存する誤差ベクトル${\bf e}( {\bf x} ) \in \mathbb{R}^{M}$を導入します．
今，複数の誤差ベクトルを用いて，以下のコスト関数を定義します．
%
\begin{align}
  E = \sum_{i} \| {\bf e}_{i} \|_{2}^{2} \in \mathbb{R}
\end{align}
%
ここで$\| \cdot \|_{2}^{2}$は，ベクトルの2乗ノルムを計算する操作になります．
そして，コスト関数を最小化する状態を以下のように定義します．
%
\begin{align}
  {\bf x}^{*} = \argmin_{ {\bf x} } E
\end{align}
%
これは，${\bf x}$の定義域でコスト関数を最小化する状態を${\bf x}^{*}$するという意味になり，このような解を{\bf 最適解}（Optimal Solution）と呼びます\footnote{しばしば，「良さそうな解」を「最適な解」と表現してしまう方が多いですが，工学的に最適な解とは最適解を意味し，最適解である以上あるコスト関数を最小化（もしくは目的関数を最大化）する解を意味してしまうことになります．最適という言葉を使うときは，必ず対応する関数を示すようにしてください．}．

ガウス・ニュートン法による最適化を考えるにあたり，まず誤差ベクトルを1次のテイラー展開で近似することを考えます．
%
\begin{align}
  {\bf e}( {\bf x} + \delta {\bf x} ) \simeq {\bf e}( {\bf x} ) + J \delta {\bf x}
  \label{eq:error_vector_taylor}
\end{align}
%
ここで$J$は誤差ベクトル${\bf e}$の状態ベクトル${\bf x}$に関する偏微分$\frac{ \partial {\bf e} }{ \partial {\bf x} } \in \mathbb{R}^{M \times N}$，すなわち{\bf ヤコビアン}（Jacobian）になります．
次に，式(\ref{eq:error_vector_taylor})の近似された誤差ベクトルの2乗ノルムを考えます．
%
\begin{align}
  \begin{split}
    & ( {\bf e} + J \delta {\bf x} )^{\top} ( {\bf e} + J \delta {\bf x} ) \\
    %
    = & ( {\bf e}^{\top} + \delta {\bf x}^{\top} J^{\top} ) ( {\bf e} + J \delta {\bf x} ) \\
    %
    = & {\bf e}^{\top} {\bf e} + {\bf e}^{\top} J \delta {\bf x} + \delta {\bf x}^{\top} J^{\top} {\bf e} + \delta {\bf x}^{\top} J^{\top} J \delta {\bf x} \\
    %
    = & {\bf e}^{\top} {\bf e} + 2 J^{\top} {\bf e} \delta {\bf x} + \delta {\bf x}^{\top} J^{\top} J \delta {\bf x}
  \end{split}
  \label{eq:error_vector_taylor_sq_norm}
\end{align}
%
なお，${\bf e}^{\top} J \delta {\bf x} = \delta {\bf x}^{\top} J^{\top} {\bf e}$を用いています．

続いて，式(\ref{eq:error_vector_taylor_sq_norm})を$\delta {\bf x}$の関数$f( \delta {\bf x} )$とみなして，$\delta {\bf x}$で偏微分します．
%
\begin{align}
  \frac{ \partial f( \delta {\bf x} ) }{ \partial \delta {\bf x} } = 2 J^{\top} {\bf e} + 2 J^{\top} J \delta {\bf x}
  \label{eq:error_vector_taylor_sq_norm_partial}
\end{align}
%
そして，式(\ref{eq:error_vector_taylor_sq_norm_partial})が${\bf 0}$になると仮定すると，次式が成り立ちます．
%
\begin{align}
  J^{\top} J \delta {\bf x} = -J^{\top} {\bf e}
  \label{eq:gauss_newton_update_value}
\end{align}

ここで，式(\ref{eq:gauss_newton_update_value})を満たす$\delta {\bf x}$について考えます．
$f( \delta {\bf x} )$は，$\delta {\bf x}$だけ誤差ベクトルを変化させたものを近似し，その2乗ノルムを計算したものになっています．
これを$\delta {\bf x}$に関して偏微分し，その結果を${\bf 0}$とすると，近似した誤差の2乗ノルムを最小にする$\delta {\bf x}$を求めることが可能になります．
すなわち，この操作で求められた$\delta {\bf x}$分だけ${\bf x}$を更新すると，コスト関数を減少させることができるようになります．

式(\ref{eq:error_vector_taylor_sq_norm})を導出するにあっては，1つの誤差ベクトルの近似を考えましたが，実際のコスト関数は複数の誤差ベクトルの2乗ノルムの和を計算しています．
そのため，状態${\bf x}$を$\delta {\bf x}$だけ変化させたコスト関数を近似する必要がありますが，これは式(\ref{eq:cost_function_taylor})のようになります．
%
\begin{align}
  E( {\bf x} + \delta {\bf x} ) \simeq \sum_{i=1}^{N} \left( {\bf e}_{i}^{\top} {\bf e}_{i} + 2 J_{i}^{\top} {\bf e}_{i} \delta {\bf x} + \delta {\bf x}^{\top} J_{i}^{\top} J_{i} \delta {\bf x} \right)
  \label{eq:cost_function_taylor}
\end{align}
%
そして同様に，式(\ref{eq:cost_function_taylor})を$\delta {\bf x}$で偏微分した結果を${\bf 0}$にすると，次式が得られます．
%
\begin{align}
  \sum_{i=1}^{N} J_{i}^{\top} J_{i} \delta {\bf x} = -\sum_{i=1}^{N} J_{i}^{\top} {\bf e}_{i}
\end{align}
%
ここで簡略化のため，以下のように変数を導入します．
%
\begin{align}
  \begin{gathered}
    H = \sum_{i=1}^{N} J_{i}^{\top} J_{i} \\
    {\bf b} = \sum_{i=1}^{N} J_{i}^{\top} {\bf e}_{i}
  \end{gathered}
\end{align}
%
すなわち，$H \delta {\bf x} = -{\bf b}$を満たす$\delta {\bf x}$を得た後に，以下のように状態を更新します．
%
\begin{align}
  {\bf x} \leftarrow {\bf x} + \delta {\bf x}
  \label{eq:gauss_newton_update_vector}
\end{align}
%
なお，$H \delta {\bf x} = -{\bf b}$からは，当然$\delta {\bf x} = -H^{-1} {\bf b}$が導けますが，$H \in \mathbb{N \times N}$になるため，$N$が大きい場合には逆行列の直接的な計算が困難になります．
そのため，直接逆行列を計算することが少ないため，「$H \delta {\bf x} = -{\bf b}$を満たす$\delta {\bf x}$」という表現を用いています．





\subsection{リー群とリー代数}

本書では頻繁に{\bf リー群}（Lie Group）と{\bf リー代数}（Lie Algebra）を用います．
リー群やリー代数の厳密な説明は行いませんが，本書でリー群と呼ぶものは{\bf 回転行列}（Rotation Matrix）と{\bf 斉次行列}（Homogeneous Transformation Matrix）になります．
回転行列は以下のように定義されます．
%
\begin{align}
  \{R \in \mathbb{R}^{3 \times 3} | R^{\top} R = I, {\rm det}(R) = 1\} \\
\end{align}
%
回転行列はSpecial Orthogonal Group in 3 Dimensions（${\rm SO}(3)$）とも呼ばれ，3次元空間の回転を表現することができます．
斉次行列は以下のように定義されます．
%
\begin{align}
  \left\{ \left( \begin{matrix} R & {\bf t} \\ {\bf 0}^{\top} & 1 \end{matrix} \right) | R \in {\rm SO}(3), {\bf t} \in \mathbb{R}^{3} \right\}
\end{align}
%
斉次行列はSpecial Euclidean Group in 3 Dimensions（${\rm SE}(3)$）とも呼ばれ，3次元空間での回転を含む位置（姿勢）を表現することができます．
LIOやSLAMでは基本的に，${\rm SO}(3)$や${\rm SE}(3)$を用いて状態を表現します．

リー群を用いる利点は，回転の状態を，途中で急に値が飛んだり切り替わったりすることなく，滑らかにかつ数学的に自然な形で扱えるということです．
直感的には少し難しく感じるかもしれませんが，まずは平面上の回転，つまり$xy$平面での角度$\theta$を例に考えてみます．
角度$\theta$は通常，$0 \leq \theta < 2\pi$（あるいは$-\pi \leq \theta < \pi$）の範囲で定義されますが，$\theta = 0$と$\theta = 2\pi$は，数値としては異なるものの，回転としては同じ状態を表しています．
このような性質から，角度$\theta$による表現では，状態が不連続に見えることがあります．
しかしリー群を使えば，回転の変化を「切れ目のない空間」で表現でき，最初から最後まで滑らかに（連続的に）扱えるようになります．
また，加減算や微分といった操作も数学的に統一されたルールで行えるので，処理が一貫してスムーズになります．

\subsubsection{反対称行列}

ある3次元ベクトルに対して{\bf 反対称行列}（Skew-Symmetric Matrix）を生成する操作を以下のように定義します．
%
\begin{align}
  {\bf a}^{^\wedge} = \left( \begin{matrix}
                        0     & -a_{z} & a_{2} \\
                        a_{z} & 0      & -a_{1} \\
                        -a_{2} & a_{1} & 0
                      \end{matrix} \right)
  \label{eq:skew_symmetric_matrix}
\end{align}


\begin{align}
  \begin{gathered}
    \theta = \| \boldsymbol \omega \|_{2} \\
    %
    \exp( [ \boldsymbol \omega ]_{\times} ) =
      I_{3} +
      \frac{ \sin \theta }{ \theta } [ \boldsymbol \omega ]_{\times} +
      \frac{ 1 - \cos \theta }{ \theta^{2} } [ \boldsymbol \omega ]_{\times}^{2}
  \end{gathered}
  \label{eq:so3_exp_map}
\end{align}
%

\begin{align}
  \begin{gathered}
    \theta = \arccos \left( \frac{{\rm tr}(R) - 1}{ 2 } \right) \\
    %
    \log( R ) = \frac{ \theta }{ 2 \sin \theta } \left( R - R^{\top} \right)
  \end{gathered}
\end{align}
%
なお$\log( R ) = \boldsymbol \omega^{\wedge} \in \mathfrak{so}(3)$となるため，この反対称行列から3次元ベクトル$\boldsymbol \omega$を取り出す操作を以下のように定義します．
%
\begin{align}
  \boldsymbol \omega = \left( \boldsymbol \omega^{\wedge} \right)^{\vee}
\end{align}


